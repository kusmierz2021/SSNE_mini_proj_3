{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def check_classes_amount(set_with_labels):\n",
    "    counter = [1,0,0,0,0,0,0,0,0,0,\n",
    "               0,0,0,0,0,0,0,0,0,0,\n",
    "               0,0,0,0,0,0,0,0,0,0,\n",
    "               0,0,0,0,0,0,0,0,0,0,\n",
    "               0,0,0,0,0,0,0,0,0,0]\n",
    "    count = 0\n",
    "    for j in tqdm(range(88010)):\n",
    "        if set_with_labels[j][1] == set_with_labels[j+1][1]:\n",
    "            counter[count] = counter[count] + 1\n",
    "        else:\n",
    "            count += 1\n",
    "            counter[count] = counter[count] + 1\n",
    "\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sample_bread_carbon():\n",
    "    classes_amounts = check_classes_amount(trainset)\n",
    "    sub_folders = os.listdir(\"./train\")\n",
    "    list_of_images_bread = os.listdir(f\"./train/{sub_folders[10]}\")\n",
    "    list_of_images_carbon = os.listdir(f\"./train/{sub_folders[13]}\")\n",
    "\n",
    "    to_oversample_bread = random.sample(list_of_images_bread, k=(1800 - classes_amounts[10]))\n",
    "    to_oversample_carbon = random.sample(list_of_images_carbon, k= (1800 - 3 * classes_amounts[13]))\n",
    "\n",
    "    # oversample bread class\n",
    "    for i in range(len(to_oversample_bread)):\n",
    "        shutil.copy(f\"./train/{sub_folders[10]}/{to_oversample_bread[i]}\", f\"./train/{sub_folders[10]}/{[name[:-5] + '_1' + name[-5:] for name in to_oversample_bread][i]}\")\n",
    "\n",
    "    # oversample carbon class\n",
    "    for i in range(classes_amounts[13]):\n",
    "        shutil.copy(f\"./train/{sub_folders[13]}/{list_of_images_carbon[i]}\", f\"./train/{sub_folders[13]}/{[name[:-5] + '_1' + name[-5:] for name in list_of_images_carbon][i]}\")\n",
    "\n",
    "    for i in range(classes_amounts[13]):\n",
    "        shutil.copy(f\"./train/{sub_folders[13]}/{list_of_images_carbon[i]}\", f\"./train/{sub_folders[13]}/{[name[:-5] + '_2' + name[-5:] for name in list_of_images_carbon][i]}\")\n",
    "\n",
    "    for i in range(len(to_oversample_carbon)):\n",
    "        shutil.copy(f\"./train/{sub_folders[13]}/{to_oversample_carbon[i]}\", f\"./train/{sub_folders[13]}/{[name[:-5] + '_3' + name[-5:] for name in to_oversample_carbon][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.ImageFolder(\"train/\")\n",
    "# over_sample_bread_carbon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create val folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_val_folder():\n",
    "    sub_folders = os.listdir(\"./train\")\n",
    "    os.mkdir(\"./val\")\n",
    "    for sub_folder in sub_folders:\n",
    "        os.mkdir(f\"./val/{sub_folder}\")\n",
    "\n",
    "    for sub_folder in sub_folders:\n",
    "        to_move = random.sample(os.listdir(f\"./train/{sub_folder}\"), k=360)\n",
    "        for image in to_move:\n",
    "            shutil.move(f\"./train/{sub_folder}/{image}\", f\"./val/{sub_folder}/{image}\")\n",
    "\n",
    "# create_val_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.RandomHorizontalFlip(0.5),\n",
    "#      transforms.RandomVerticalFlip(0.5),\n",
    "#      transforms.RandomRotation(24),\n",
    "#      transforms.ToTensor(),\n",
    "#      transforms.RandomErasing(0.4),\n",
    "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.ImageFolder(\"train/\", transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "valset = datasets.ImageFolder(\"val/\", transform=transform)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = datasets.ImageFolder(\"test_all/\", transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "classes = tuple(trainset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125\n",
      "282\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader))\n",
    "print(len(valloader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# plt.figure(figsize = (20,10))\n",
    "#\n",
    "# # get some random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "#\n",
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "# # get some random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "#\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "#\n",
    "# print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images[30:31].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images).cpu()\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Net(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(64, 250, kernel_size=(3, 3), stride=(1, 1))\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(250, 250, kernel_size=(3, 3), stride=(1, 1))\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (drop): Dropout(p=0.5, inplace=False)\n  (conv4): Conv2d(250, 250, kernel_size=(3, 3), stride=(1, 1))\n  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc0): Linear(in_features=1000, out_features=500, bias=True)\n  (fc2): Linear(in_features=500, out_features=250, bias=True)\n  (fc3): Linear(in_features=250, out_features=150, bias=True)\n  (fc4): Linear(in_features=150, out_features=50, bias=True)\n)"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "##Warstwakonwolucyjna\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3,stride=1,padding=0)\n",
    "##Warstwamaxpooling\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2 = nn.Conv2d(64,250,3)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(250, 250, 3)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.conv4 = nn.Conv2d(250, 250, 3)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc0 = nn.Linear(1000,500)\n",
    "        # self.fc1 = nn.Linear(2200,1100)\n",
    "        self.fc2 = nn.Linear(500,250)\n",
    "        self.fc3 = nn.Linear(250, 150)\n",
    "        self.fc4 = nn.Linear(150,50)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x,1)#flattenalldimensionsexceptbatch\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1125it [01:48, 10.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/8] loss: 1.895\n",
      "accuracy on trainloader: 49.34583333333333\n",
      "accuracy on valloader: 43.32222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1125it [01:47, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/8] loss: 1.838\n",
      "accuracy on trainloader: 51.12777777777778\n",
      "accuracy on valloader: 44.44444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1125it [01:49, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/8] loss: 1.786\n",
      "accuracy on trainloader: 51.705555555555556\n",
      "accuracy on valloader: 44.62777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1125it [01:32, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/8] loss: 1.743\n",
      "accuracy on trainloader: 52.834722222222226\n",
      "accuracy on valloader: 45.25555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1125it [01:27, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/8] loss: 1.706\n",
      "accuracy on trainloader: 54.86666666666667\n",
      "accuracy on valloader: 46.18888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1125it [01:28, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/8] loss: 1.669\n",
      "accuracy on trainloader: 54.30833333333333\n",
      "accuracy on valloader: 45.46111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1125it [01:28, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/8] loss: 1.642\n",
      "accuracy on trainloader: 55.65555555555556\n",
      "accuracy on valloader: 45.91111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1125it [01:28, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/8] loss: 1.614\n",
      "accuracy on trainloader: 57.15972222222222\n",
      "accuracy on valloader: 46.37777777777778\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 8\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(trainloader, 0)):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print('[%d/%d] loss: %.3f' %\n",
    "          (epoch+1, epochs,  running_loss / len(trainloader)))\n",
    "    running_loss = 0.0\n",
    "\n",
    "\n",
    "    print(f\"accuracy on trainloader: {get_accuracy(trainloader)}\")\n",
    "    print(f\"accuracy on valloader: {get_accuracy(valloader)}\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "# images = images[10:20]\n",
    "\n",
    "# print images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images.to(device))\n",
    "#outputs\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(len(predicted))))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making predictions to .csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images).cpu()\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # print(predicted)\n",
    "        for i in predicted:\n",
    "            predictions.append(i.item())\n",
    "\n",
    "# print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "#     100 * correct / total))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0\n",
      "0  10\n",
      "1  31\n",
      "2  15\n",
      "3  39\n",
      "4  45\n"
     ]
    }
   ],
   "source": [
    "list_of_test_img = os.listdir(f\"./test_all/test_all\")\n",
    "img_name = pd.DataFrame(list_of_test_img)\n",
    "# print(list_of_test_img)\n",
    "pred = pd.DataFrame(predictions)\n",
    "print(pred.head())\n",
    "results = pd.concat([img_name, pred], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(img_name[0].unique()))\n",
    "print(len(pred[0].unique()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "results.to_csv('Kusmierz_Podolec_newest.csv', header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
